---
title: "question3"
author: "Ermioni Athanasiadi"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
############################################################
# PACKAGES REQUIRED FOR QUESTION 3
############################################################

libraries_q3 <- c(
  "haven",      # read_sas
  "dplyr",      # data manipulation
  "tidyr",      # pivot_longer
  "ggplot2",    # plots
  "nlme",       # gls(), correlation structures
  "lattice",    # diagnostic plots
  "sjPlot",     # tab_model
  "xtable",     # latex table output
  "kableExtra"  # styled tables
)

# Install missing packages
for (pkg in libraries_q3) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}

# Load packages
invisible(lapply(libraries_q3, library, character.only = TRUE))


############################################################
# LOAD DATA (SELF-CONTAINED)
############################################################

setwd("./../data")
dat <- read_sas("alzheimer25.sas7bdat")


############################################################
# CANONICAL DATA PREPARATION (MATCH Q1 & Q2)
############################################################

reshape_dat <- function(dat) {
  dat_long <- dat %>%
    pivot_longer(
      cols = matches("^(bprs|cdrsb|abpet|taupet)\\d+"),
      names_to = c(".value", "time"),
      names_pattern = "(.+)(\\d+)"
    )
  dat_long
}

to_factor <- function(dat_long) {
  dat_long <- dat_long %>%
    mutate(
      time     = as.factor(time),     # coded 0â€“6
      sex      = as.factor(sex),
      edu      = as.factor(edu),
      trial    = as.factor(trial),
      job      = as.factor(job),
      wzc      = as.factor(wzc)
    )
  dat_long
}

# Create long-format dataset
dat_long <- reshape_dat(dat)
dat_long <- to_factor(dat_long)
```

```{r}
# Baseline per subject (year 0)
base <- dat_long %>%
  filter(time == 0) %>%
  transmute(
    patid, trial,
    cdrsb.0 = cdrsb, 
    abpet.0 = abpet, 
    taupet.0 = taupet
  )

# Add baseline values to all rows
dat_long <- dat_long %>%
  left_join(base, by = c("patid", "trial")) %>%
  mutate(
    patid = as.factor(patid),
    time_num = as.integer(as.character(time))  # Actual time values (0-6)
  )

# Create consecutive integer variable for corSymm (required: must be 1, 2, 3, ...)
# Map unique time values to consecutive integers starting from 1
time_levels <- sort(unique(dat_long$time_num))
time_mapping <- setNames(seq_along(time_levels), as.character(time_levels))
dat_long$time_seq <- as.integer(time_mapping[as.character(dat_long$time_num)])

rm(base)
```

# Q3: Multivariate Model

_Q3. Fit a multivariate model, and find the most parsimonious mean structure which can be used to describe the average evolutions in the data. What covariance structures are applicable in this case ? What is the most parsimonious structure you can find?_

```{r}
form_full <- bprs ~ time_num + age + sex + trial + edu + bmi + inkomen + job + adl + wzc + cdrsb.0 + abpet.0 + taupet.0 

m_full <- gls(
  model = form_full,
  weights = varIdent(form = ~ 1 | time_num),
  method = "ML",
  data = dat_long,
  na.action = na.exclude
)

summary(m_full)
```

```{r}
dat_long$resid_p <- residuals(m_full, type = "pearson")
dat_long$resid_n <- residuals(m_full, type = "normalized")

ggplot(dat_long, aes(x = factor(time_num), y = resid_p)) +
  geom_jitter(alpha = 0.2, width = 0.15) +
  geom_boxplot(outlier.shape = NA, alpha = 0.6) +
  labs(x = "Year", y = "Pearson residuals", title = "Residuals by time level") +
  theme_bw()
ggsave("./../figures/q3_resid_p.png", width = 8, height = 6, dpi = 300)

ggplot(dat_long, aes(x = factor(time_num), y = resid_n)) +
  geom_jitter(alpha = 0.2, width = 0.15) +
  geom_boxplot(outlier.shape = NA, alpha = 0.6) +
  labs(x = "Year", y = "Normalized residuals", title = "Residuals by time level") +
  theme_bw()
ggsave("./../figures/q3_resid_n.png", width = 8, height = 6, dpi = 300)

ggplot(dat_long, aes(x = fitted(m_full), y = resid_n)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0) +
  labs(x = "Fitted values", y = "Normalized residuals", title = "Standardized residuals vs fitted") +
  theme_bw() 
ggsave("./../figures/q3_resid_vs_fitted.png", width = 8, height = 6, dpi = 300)

q3_resid_vs_fitted_by_year <- plot(m_full, resid(., type = "p") ~ fitted(.) | as.factor(time_num),
     abline = 0, ylab = "Pearson residuals")
png("./../figures/q3_resid_vs_fitted_by_year.png")
print(q3_resid_vs_fitted_by_year)
dev.off()
q3_resid_vs_fitted_by_year
```

```{r}
m0 <- gls(
  model = bprs ~ 1,
  weights = varIdent(form = ~ 1 | time_num),
  data = dat_long,
  method = "ML",
  na.action = na.exclude
)

anova(m0, m_full)
```

```{r}
tab_model(m_full, file = "model.html", show.r2 = TRUE, show.se = TRUE)
xtable(summary(m_full)$tTable)
```


#### Selection of Fixed effects
With our preliminary mean structure, we will now try to reduce the model stepwise:
Lets start with the main effects:
##### Main effects
```{r}
m1 <- m_full

m2 <- update(m1, . ~ . - inkomen)
a1 <- anova(m1, m2)  
m1 <- m2 # exclude inkomen

m3 <- update(m1, . ~ . - bmi)
a2 <- anova(m1, m3)

m4 <- update(m1, . ~ . - job)
a3 <- anova(m1, m4) 

m5 <- update(m1, . ~ . - edu)
a4 <- anova(m1, m5)
m1 <- m5 # exclude edu

m6 <- update(m1, . ~ . - wzc)
a5 <- anova(m1, m6)

m7 <- update(m1, . ~ . - adl)
a6 <- anova(m1, m7)
m1 <- m7 # exclude adl

m8 <- update(m1, . ~ . - trial)
a7 <- anova(m1, m8)

m9 <- update(m1, . ~ . - taupet.0)
a8 <- anova(m1, m9)
m1 <- m9  # exclude taupet

m10 <- update(m1, . ~ . - abpet.0)
a9 <- anova(m1, m10)
m1 <- m10 # exclude abpet

m11 <- update(m1, . ~ . - sex)
a10 <- anova(m1, m11)
m1 <- m11  # exclude sex

m12 <- update(m1, . ~ . - age)
a11 <- anova(m1, m12)

m13 <- update(m1, . ~ . - time_num)
a12 <- anova(m1, m13)

m14 <- update(m1, . ~ . - cdrsb.0)
a13 <- anova(m1, m14)

```

The strongest Likelihood Ratios are obtained for time, age and trial. 
We only exclude inkomen and sex as predictors.
*Update* We can also exclude 
- edu
- adl
- taupet.0
- abpet.0



So the most parsimonious model based on Likelihood Inference looks like this:

bprs ~ time_num + age + trial + bmi + job + wzc + cdrsb.0


###### Export
```{r}

anovas <- mget(paste0("a", seq(1:13)))

anovas_df <- lapply(anovas, function(x) {
    df <- as.data.frame(x)
    df$model <- rownames(x)
    df
    })

tab_anovas <- bind_rows(anovas_df)
rownames(tab_anovas) <- NULL


tab_anovas <- tab_anovas %>% 
    select(-Model, -call) %>% 
    relocate(model, .before = "df") %>%
    mutate(

        p.value.form = ifelse(`p-value` < .001, "< .001", sprintf("%.3f", tab_anovas$`p-value`)),
        across(where(~is.numeric(.)), ~ round(., 2))
    ) %>%
    select(-`p-value`)

tab_anovas <- tab_anovas %>% 
    filter((!is.na(L.Ratio) & !is.na(p.value.form)) | row_number()==1  )


q3_tab_anovas_latex <- tab_anovas %>% 
    kbl(format = "latex", 
                   booktabs = TRUE, 
                   caption = "Likelihood Ratio Tests used for model selection of main effects ",
                   label = "tab:q3_anova_main_effects"
                       ) %>%
    kable_styling(latex_options = c("hold_position"))

```

```{r}
# remove objects not needed anymore
rm(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13)
```


```{r}
form_fixed <- bprs ~ time_num + age + trial + bmi + job + wzc + cdrsb.0
m1 <- gls(
  model = form_fixed, 
  weights = varIdent(form = ~ 1 | time_num),
  method = "REML",
  data = dat_long,
  na.action = na.exclude
)

summary(m1)
```


#### Selection of Covariance Structure

Possible Covariance and Variance Specifications:
 (with ML or REML)
- using only patientid for random intercepts
- Using nested patid in trial
- removing the heterogenous variance weights
- removing the nested trial in the correlation structure AND the variance weights
- allowing separate covariances for different years
- readding the weights

Note about form of the Cor structure:
correlation structure is assumed to apply only to observations within the same grouping level (i.e. within the same patient/trial); observations with different grouping levels (patients) are assumed to be uncorrelated


## Selection of Covariance Structure

```{r}
m2 <- gls(
  model = form_fixed, 
  method = "REML",
  data = dat_long,
  na.action = na.exclude
)

summary(m2)
anova(m1, m2)
```
We tested if the model with weights is indeed better.
Ok, we know fitting different variances per year does improve our model fit. Lets stick with this model.

(..........)

```{r}
m3 <- gls(
  model = form_fixed, 
  weights = varIdent(form = ~ 1 | time_num),
  correlation = corSymm(form = ~ time_seq | patid),
  method = "REML",
  data = dat_long,
  na.action = na.exclude
)

summary(m3)
anova(m1, m3)
m1 <- m3
```
This improves our model, as expected. we keep the unstructured covariance.



```{r}
m4 <- gls(
  model = form_fixed, 
  correlation = corSymm(form = ~ time_seq | patid),
  method = "REML",
  data = dat_long,
  na.action = na.exclude
)

summary(m4)
anova(m1, m4)

```

Removing the weights is worse. We keep our model.

Nested factor trial/patid --> this is not possible in the correlation matrix!!!
Because we can only model WITHIN-patient correlations, while the nested structure of patients between different trials (and wanting to estimate how much patients of different trial centers differ) is obviously BETWEEN subjects correlation.

Thus, we can ignore this in the multivariate model. This will only become significant for random effect estimation.

```{r}
# check out the variance
m1$modelStruct$corStruct
```
The correlations dont really decrease that much with increasing lag. Although there is a small trend, but for lag(1,7) its again stronger than for lag(1,6) so we cant be sure. But errors become more correlated the later the time point is.

```{r}
m5 <- gls(
  model = form_fixed, 
  weights = varIdent(form = ~ 1 | time_num),
  correlation = corCompSymm(form = ~ time_seq | patid),
  method = "REML",
  data = dat_long,
  na.action = na.exclude
)


summary(m5)
anova(m1, m5)
```

The model is worse. So no compound symmetry 

```{r}
m6 <- gls(
  model = form_fixed, 
  weights = varIdent(form = ~ 1 | time_num),
  correlation = corAR1(form = ~ time_seq | patid),
  method = "REML",
  data = dat_long,
  na.action = na.exclude
)



summary(m6)
anova(m1, m6)
```

As we observed earlier, the corr decreases with more lag, but not consistently, for last time point its again stronger. So the AR1 strcture is not better.


The covariance matrix indicates decreasing trends but in the last years increasing again -> explanation?

```{r}
m7 <- gls(
  model = form_fixed, 
  weights = varIdent(form = ~ 1 | time_num),
  correlation = corARMA(form = ~ time_seq | patid, p = 6, q = 0),
  method = "REML",
  data = dat_long,
  na.action = na.exclude
)


summary(m7)
anova(m1, m7)
```

This model still has worse fit, but not that much worse.

ARMA is a combination of AR und MA models
AR = autoregressive model = dependency of time points on earlier time points
MA = moving average, considers weighted averages of past errors

```{r}
anova(m2, m3, m4, m5, m6, m7)
```
The most parsimonious structure is the unstructured one. All more parsimonious structures don't reach the same model fit.




## Examine Covariance Matrices

```{r}
get_cor_matrix <- function(mod) {
  cor_pars <- coef(mod$modelStruct$corStruct, unconstrained = FALSE)
  var_pars <- coef(mod$modelStruct$varStruct, unconstrained = FALSE)
  var_pars <- c("1" = 1, var_pars)
  sigma0 <- mod$sigma
  k <- length(var_pars)
  C <- diag(1, k)
  C[lower.tri(C)] <- cor_pars
  C[upper.tri(C)] <- t(C)[upper.tri(C)]
  C <- round(C, 3)
  C
}

get_cov_matrix <- function(mod) {
  var_pars <- coef(mod$modelStruct$varStruct, unconstrained = FALSE)
  var_pars <- c("1" = 1, var_pars)
  sigma0 <- mod$sigma
  sd_vars <- sigma0 * sqrt(var_pars)
  D <- diag(sd_vars)
  C <- get_cor_matrix(mod)
  S <- D %*% C %*% D
  S <- round(S, 3)
  S
}

get_cor_matrix(m1)
get_cov_matrix(m1)
```

```{r}
models <- list(m5, m6, m7)
lapply(models, get_cor_matrix)
lapply(models, get_cov_matrix)
```



